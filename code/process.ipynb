{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_path = \"./test/{}.parquet\"\n",
    "tasks = [\"olympiad_bench\", \"minerva\"]\n",
    "\n",
    "for task in tasks:\n",
    "    # Read the parquet file\n",
    "    df = pd.read_parquet(input_path.format(task))\n",
    "\n",
    "    ans_list = df[\"reward_model\"].tolist()\n",
    "    for ans in ans_list:\n",
    "        ans[\"ground_truth\"] = ans[\"ground_truth\"].item() if isinstance(ans, np.ndarray) and ans.size == 1 else ans\n",
    "\n",
    "\n",
    "    # 检查并替换 np.array 为 item\n",
    "    df[\"reward_model\"] = ans_list\n",
    "\n",
    "    \n",
    "    df.to_csv(input_path.format(task), index=False)\n",
    "    \n",
    "    print(f\"Saved {task} to {output_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "input_path = \"./test/{}.parquet\"\n",
    "tasks = [\"olympiad_bench\", \"minerva\", \"aime\", \"amc\", \"math\", \"olympiad_bench\", \"college\", \"theoremqa\"]\n",
    "del_strs = \" Let's think step by step and output the final answer within \\\\boxed{}.\"\n",
    "\n",
    "for task in tasks:\n",
    "    # Read the parquet file\n",
    "    input_path = f\"./test/{task}.parquet\"\n",
    "\n",
    "    df = pd.read_parquet(input_path)\n",
    "    output_path = f\"./test/{task}.json\"\n",
    "    df.to_json(output_path, orient=\"records\", lines=True)\n",
    "\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as fr:\n",
    "        data_pool = [json.loads(line) for line in fr.readlines()]\n",
    "\n",
    "    for data in data_pool:\n",
    "        data[\"id\"] = data[\"extra_info\"][\"index\"]\n",
    "        data[\"data_source\"] = task\n",
    "        assert len(data[\"prompt\"]) == 1\n",
    "        data[\"question\"] = data[\"prompt\"][0][\"content\"].replace(del_strs, \"\")\n",
    "        del data[\"prompt\"]\n",
    "        del data[\"ability\"]\n",
    "        if isinstance(data[\"reward_model\"][\"ground_truth\"], np.ndarray):\n",
    "            data[\"reward_model\"][\"ground_truth\"] = data[\"reward_model\"][\"ground_truth\"].item()\n",
    "        data[\"ground_truth\"] = data[\"reward_model\"][\"ground_truth\"]\n",
    "        del data[\"reward_model\"]\n",
    "        del data[\"extra_info\"]\n",
    "    \n",
    "    with open(output_path, \"w\") as fw:\n",
    "        json.dump(data_pool, fw, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"./test/aime24.jsonl\"\n",
    "data_path = \"./test/aime.json\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as fr:\n",
    "    data_pool = [json.loads(line) for line in fr.readlines()]\n",
    "\n",
    "data_list = json.load(open(data_path, \"r\", encoding=\"utf-8\"))\n",
    "for idx, data in enumerate(data_pool):\n",
    "    for data_item in data_list:\n",
    "        if data[\"question\"] == data_item[\"question\"]:\n",
    "            data_item[\"solution\"] = data[\"solution\"]\n",
    "\n",
    "with open(data_path, \"w\", encoding=\"utf-8\") as fw:\n",
    "    json.dump(data_list, fw, indent=4)\n",
    "\n",
    "with open(input_path, \"w\", encoding=\"utf-8\") as fw:\n",
    "    json.dump(data_pool, fw, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "input_path = \"./test/math.json\"\n",
    "output_path = \"./test/math100.json\"\n",
    "\n",
    "dataset = json.load(open(input_path, \"r\", encoding=\"utf-8\"))\n",
    "new_dataset = random.sample(dataset, 100)\n",
    "\n",
    "sorted_data = sorted(new_dataset, key=lambda x: x['id'])\n",
    "\n",
    "json.dump(sorted_data, open(output_path, \"w\", encoding=\"utf-8\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: aime, Suffix: 4o_4o, Remove Count: 85, Contradict Count: 85\n",
      "------------------------------\n",
      "Dataset: aime, Suffix: r1_r1, Remove Count: 85, Contradict Count: 85\n",
      "------------------------------\n",
      "Dataset: aime, Suffix: 4o_r1, Remove Count: 85, Contradict Count: 85\n",
      "------------------------------\n",
      "Dataset: aime, Suffix: r1_4o, Remove Count: 85, Contradict Count: 85\n",
      "------------------------------\n",
      "Dataset: amc, Suffix: 4o_4o, Remove Count: 241, Contradict Count: 241\n",
      "------------------------------\n",
      "Error decoding JSON in file: ./unsol/v4-comp/amc_rewrite_r1_r1.json\n",
      "Error decoding JSON in file: ./unsol/v4-comp/amc_rewrite_4o_r1.json\n",
      "Dataset: amc, Suffix: r1_4o, Remove Count: 241, Contradict Count: 241\n",
      "------------------------------\n",
      "Dataset: math100, Suffix: 4o_4o, Remove Count: 254, Contradict Count: 254\n",
      "------------------------------\n",
      "Dataset: math100, Suffix: r1_r1, Remove Count: 254, Contradict Count: 254\n",
      "------------------------------\n",
      "Error decoding JSON in file: ./unsol/v4-comp/math100_rewrite_4o_r1.json\n",
      "Dataset: math100, Suffix: r1_4o, Remove Count: 254, Contradict Count: 254\n",
      "------------------------------\n",
      "Dataset: minerva100, Suffix: 4o_4o, Remove Count: 273, Contradict Count: 273\n",
      "------------------------------\n",
      "Dataset: minerva100, Suffix: r1_r1, Remove Count: 273, Contradict Count: 273\n",
      "------------------------------\n",
      "Dataset: minerva100, Suffix: 4o_r1, Remove Count: 273, Contradict Count: 273\n",
      "------------------------------\n",
      "Error decoding JSON in file: ./unsol/v4-comp/minerva100_rewrite_r1_4o.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"./unsol/v4-comp/{}_rewrite_{}.json\"\n",
    "datasets = [\"aime\", \"amc\", \"math100\", \"minerva100\"]\n",
    "suffixes = [\"4o_4o\", \"r1_r1\", \"4o_r1\", \"r1_4o\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for suffix in suffixes:\n",
    "        remove_count, contradict_count = 0, 0\n",
    "        input_file = input_path.format(dataset, suffix)\n",
    "        try:\n",
    "            with open(input_file, \"r\", encoding=\"utf-8\") as fr:\n",
    "                data_pool = json.load(fr)\n",
    "            for data in data_pool:\n",
    "                remove_count += len(data[\"remove\"])\n",
    "                contradict_count += len(data[\"contradict\"])\n",
    "            print(f\"Dataset: {dataset}, Suffix: {suffix}, Remove Count: {remove_count}, Contradict Count: {contradict_count}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON in file: {input_file}\")\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {input_file}\")\n",
    "            continue\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文本: 1. sentence1\n",
      "\n",
      "2. sentence2\n",
      "\n",
      "3. sentence3\n",
      "处理结果: ['sentence1', 'sentence2', 'sentence3']\n",
      "--------------------------------------------------\n",
      "原文本: sentence1\n",
      "\n",
      "sentence2\n",
      "\n",
      "sentence3\n",
      "处理结果: ['sentence1', 'sentence2', 'sentence3']\n",
      "--------------------------------------------------\n",
      "原文本: 1. only one sentence\n",
      "处理结果: ['only one sentence']\n",
      "--------------------------------------------------\n",
      "原文本: first\n",
      "\n",
      "second\n",
      "处理结果: ['first', 'second']\n",
      "--------------------------------------------------\n",
      "原文本: 1. first\n",
      "\n",
      "plain second\n",
      "处理结果: ['first', 'plain second']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def extract_sentences(text):\n",
    "    # 按 \\n\\n 分割文本\n",
    "    sentences = text.split('\\n\\n')\n",
    "    \n",
    "    # 处理每个句子，去除可能存在的序号\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # 去除首尾空白\n",
    "        sentence = sentence.strip()\n",
    "        \n",
    "        # 检查并去除序号（匹配形如 \"1. \", \"2. \" 的模式）\n",
    "        if sentence and sentence[0].isdigit() and len(sentence) > 2 and sentence[1] == '.' and sentence[2] == ' ':\n",
    "            sentence = sentence[3:]\n",
    "        \n",
    "        # 将处理后的句子添加到列表中\n",
    "        if sentence:  # 确保句子非空\n",
    "            cleaned_sentences.append(sentence)\n",
    "    \n",
    "    return cleaned_sentences\n",
    "\n",
    "# 测试示例\n",
    "test_cases = [\n",
    "    \"1. sentence1\\n\\n2. sentence2\\n\\n3. sentence3\",\n",
    "    \"sentence1\\n\\nsentence2\\n\\nsentence3\",\n",
    "    \"1. only one sentence\",\n",
    "    \"first\\n\\nsecond\",\n",
    "    \"1. first\\n\\nplain second\"\n",
    "]\n",
    "\n",
    "for test in test_cases:\n",
    "    result = extract_sentences(test)\n",
    "    print(f\"原文本: {test}\")\n",
    "    print(f\"处理结果: {result}\")\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/aime_extract.json\"\n",
    "\n",
    "dataset = json.load(open(input_path, \"r\", encoding=\"utf-8\"))\n",
    "for data in dataset:\n",
    "    conditions = data[\"extracted_condition\"]\n",
    "    conditions = \"\\n\\n\".join(conditions)\n",
    "    text = conditions.replace('\\\\n\\\\n', '\\n\\n')\n",
    "\n",
    "    sentences = text.split('\\n\\n')\n",
    "\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # 去除首尾空白\n",
    "        sentence = sentence.strip()\n",
    "        \n",
    "        # 检查并去除序号（匹配形如 \"1. \", \"2. \" 的模式）\n",
    "        if sentence and sentence[0].isdigit() and len(sentence) > 2 and sentence[1] == '.' and sentence[2] == ' ':\n",
    "            sentence = sentence[3:]\n",
    "        \n",
    "        # 将处理后的句子添加到列表中\n",
    "        if sentence:  # 确保句子非空\n",
    "            cleaned_sentences.append(sentence)\n",
    "\n",
    "    data[\"extracted_condition\"] = cleaned_sentences\n",
    "\n",
    "json.dump(dataset, open(input_path, \"w\", encoding=\"utf-8\"), indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import json\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/amc_rewrite_4o_r1.json\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(data) for data in f.readlines()]\n",
    "\n",
    "write_json(input_path, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: aime, New Data Count: 141\n",
      "Dataset: amc, New Data Count: 377\n",
      "Dataset: math, New Data Count: 408\n",
      "Dataset: minerva, New Data Count: 449\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/{}_check.json\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/{}_excel.json\"\n",
    "id_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v5-human/{}_id.json\"\n",
    "datasets = [\"aime\", \"amc\", \"math\", \"minerva\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    input_file = input_path.format(dataset)\n",
    "    try:\n",
    "        data_pool = read_json(input_file)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON in file: {input_file}\")\n",
    "        continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {input_file}\")\n",
    "        continue\n",
    "    \n",
    "    new_data_list = []\n",
    "    data_id_dict = []\n",
    "    for idx, data in enumerate(data_pool):\n",
    "        if dataset in [\"math\", \"minerva\"]:\n",
    "            data_id_dict.append({\"idx\": data[\"id\"]})\n",
    "        for unsolve_type in UNS_TYPE:\n",
    "            count = 0\n",
    "            for key in data.keys():\n",
    "                if key.startswith(unsolve_type + \"_question_\"):\n",
    "                    new_data = {\n",
    "                        \"data_id\": data[\"data_source\"] + \"_\" + str(idx) + \"_\"  + unsolve_type + \"_\" + str(count+1),\n",
    "                        # \"data_source\": data[\"data_source\"],\n",
    "                        \"question\": data[\"question\"],\n",
    "                        \"ground_truth\": data[\"ground_truth\"],\n",
    "                        \"solution\": data[\"solution\"] if dataset in [\"math\", \"aime\"] else None,\n",
    "                        # \"unsolve_id\": unsolve_type + \"_\" + str(count+1),\n",
    "                        \"rewritten_question\": data[unsolve_type + \"_question_\" + str(count+1)][unsolve_type + \"_question\"],\n",
    "                        \"rewritten_condition:\": data[unsolve_type + \"_question_\" + str(count+1)][\"rewritten_condition\"],\n",
    "                        \"unsolvable_reason\": data[unsolve_type + \"_question_\" + str(count+1)][\"unsolvable_reason\"],\n",
    "                        \"human_check\": None,\n",
    "                        \"difficulty_eval\": None,\n",
    "                    }\n",
    "\n",
    "                    # if dataset in [\"math\", \"aime\"]:\n",
    "                    #     new_data[\"solution\"] = data[\"solution\"]\n",
    "                    count += 1\n",
    "                    new_data_list.append(new_data)\n",
    "\n",
    "    if dataset in [\"math\", \"minerva\"]:\n",
    "        id_file = id_path.format(dataset)\n",
    "        with open(id_file, \"w\", encoding=\"utf-8\") as fw:\n",
    "            json.dump(data_id_dict, fw, indent=4)\n",
    "    print(f\"Dataset: {dataset}, New Data Count: {len(new_data_list)}\")\n",
    "    # Save the new data to a JSON file\n",
    "    output_file = output_path.format(dataset)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as fw:\n",
    "        json.dump(new_data_list, fw, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aime to /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/aime_excel.xlsx\n",
      "Saved amc to /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/amc_excel.xlsx\n",
      "Saved math to /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/math_excel.xlsx\n",
      "Saved minerva to /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/minerva_excel.xlsx\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/{}_excel.json\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/{}_excel.xlsx\"\n",
    "datasets = [\"aime\", \"amc\", \"math\", \"minerva\"]\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    input_file = input_path.format(dataset)\n",
    "    try:\n",
    "        data_pool = read_json(input_file)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON in file: {input_file}\")\n",
    "        continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # 将数据转换为DataFrame\n",
    "    df = pd.DataFrame(data_pool)\n",
    "\n",
    "    # 保存为Excel文件\n",
    "    output_file = output_path.format(dataset)\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Saved {dataset} to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start id: 81, End id: 83\n",
      "Dataset: math, Data Size: 3, Remove Count: 7, Contradict Count: 8\n",
      "Dataset: minerva, Data Size: 3, Remove Count: 9, Contradict Count: 9\n",
      "Total Remove Count: 16, Total Contradict Count: 17, Total Count: 33\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/{}_check.json\"\n",
    "datasets = [\"math\", \"minerva\"]\n",
    "\n",
    "start, end = 81, 84\n",
    "# bias = 25\n",
    "# start += bias\n",
    "# end += bias\n",
    "remove_total, contradict_total = 0, 0\n",
    "\n",
    "print(f\"Start id: {start}, End id: {end-1}\")\n",
    "for dataset in datasets:\n",
    "    remove_count, contradict_count = 0, 0\n",
    "    input_file = input_path.format(dataset)\n",
    "    try:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as fr:\n",
    "            if dataset in [\"math\", \"minerva\"]:\n",
    "                data_pool = json.load(fr)[start:end]\n",
    "            else:\n",
    "                data_pool = json.load(fr)[start:end]\n",
    "            # data_pool = json.load(fr)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON in file: {input_file}\")\n",
    "        continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {input_file}\")\n",
    "        continue\n",
    "    for idx, data in enumerate(data_pool):\n",
    "        remove_count += len([key for key in data.keys() if key.startswith(\"remove_question_\")])\n",
    "        contradict_count += len([key for key in data.keys() if key.startswith(\"contradict_question_\")])\n",
    "    remove_total += remove_count\n",
    "    contradict_total += contradict_count\n",
    "    # print(len(data_pool))\n",
    "    print(f\"Dataset: {dataset}, Data Size: {len(data_pool)}, \" \\\n",
    "          f\"Remove Count: {remove_count}, Contradict Count: {contradict_count}\")\n",
    "print(f\"Total Remove Count: {remove_total}, Total Contradict Count: {contradict_total}, Total Count: {remove_total + contradict_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/test/math.jsonl\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/test/math.json\"\n",
    "\n",
    "data_pool = read_jsonl(input_path)\n",
    "new_data_list = read_json(output_path)\n",
    "\n",
    "for idx, data in enumerate(data_pool):\n",
    "    assert data[\"problem\"] == new_data_list[idx][\"question\"]\n",
    "    new_data_list[idx][\"solution\"] = data[\"solution\"]\n",
    "\n",
    "write_json(output_path, new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/test/math.json\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/test/math100.json\"\n",
    "\n",
    "data_pool = read_json(input_path)\n",
    "new_data_list = read_json(output_path)\n",
    "\n",
    "p1, p2 = 0, 0\n",
    "\n",
    "while p1 < len(data_pool):\n",
    "    if p2 >= len(new_data_list):\n",
    "        break\n",
    "    if new_data_list[p2][\"id\"] == p1:\n",
    "        assert new_data_list[p2][\"id\"] == data_pool[p1][\"id\"]\n",
    "        assert new_data_list[p2][\"question\"] == data_pool[p1][\"question\"]\n",
    "        new_data_list[p2][\"solution\"] = data_pool[p1][\"solution\"]\n",
    "        p2 += 1\n",
    "    p1 += 1\n",
    "\n",
    "write_json(output_path, new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/test/math100.json\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/math_check.json\"\n",
    "\n",
    "data_pool = read_json(input_path)\n",
    "new_data_list = read_json(output_path)\n",
    "\n",
    "for idx, data in enumerate(data_pool):\n",
    "    assert new_data_list[idx][\"id\"] == data[\"id\"]\n",
    "    assert data[\"question\"] == new_data_list[idx][\"question\"]\n",
    "    new_data_list[idx][\"solution\"] = data[\"solution\"]\n",
    "\n",
    "write_json(output_path, new_data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "1379\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v5-human/count.txt\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as fr:\n",
    "    data = fr.readlines()\n",
    "    data = [int(line.strip()) for line in data]\n",
    "\n",
    "np.sum(data)\n",
    "print(len(data))\n",
    "print(np.sum(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Wang Rui, Good/Passed/Discarded: 21/8/12, Total: 41, Passed Rate: 0.7073170731707317\n",
      "Name: Peng Juewen, Good/Passed/Discarded: 27/2/5, Total: 34, Passed Rate: 0.8529411764705882\n",
      "Name: Song Jiale, Good/Passed/Discarded: 11/11/10, Total: 32, Passed Rate: 0.6875\n",
      "Name: Li Ang, Good/Passed/Discarded: 19/8/5, Total: 32, Passed Rate: 0.84375\n",
      "Name: Ran Chen, Good/Passed/Discarded: 20/9/6, Total: 35, Passed Rate: 0.8285714285714286\n",
      "Name: Li Miaomiao, Good/Passed/Discarded: 28/3/4, Total: 35, Passed Rate: 0.8857142857142857\n",
      "Name: Deng Rui, Good/Passed/Discarded: 9/22/7, Total: 38, Passed Rate: 0.8157894736842105\n",
      "Name: Yu Erxin, Good/Passed/Discarded: 25/9/6, Total: 40, Passed Rate: 0.85\n",
      "Name: Wang Hongru, Good/Passed/Discarded: 16/19/4, Total: 39, Passed Rate: 0.8974358974358975\n",
      "Name: Du Yiming, Good/Passed/Discarded: 22/7/7, Total: 36, Passed Rate: 0.8055555555555556\n",
      "Name: Wang Zige, Good/Passed/Discarded: 15/15/8, Total: 38, Passed Rate: 0.7894736842105263\n",
      "Name: Zhang Zhiwei, Good/Passed/Discarded: 24/9/3, Total: 36, Passed Rate: 0.9166666666666666\n",
      "Name: Xue Boyang, Good/Passed/Discarded: 30/51/9, Total: 90, Passed Rate: 0.9\n",
      "Name: Wang Zezhong, Good/Passed/Discarded: 17/15/6, Total: 38, Passed Rate: 0.8421052631578947\n",
      "Name: Kang Jiawen, Good/Passed/Discarded: 15/21/2, Total: 38, Passed Rate: 0.9473684210526315\n",
      "Name: Zhu Qi, Good/Passed/Discarded: 20/11/9, Total: 40, Passed Rate: 0.775\n",
      "Name: Xu Hongling, Good/Passed/Discarded: 21/11/4, Total: 36, Passed Rate: 0.8888888888888888\n",
      "Name: Hu Shujie, Good/Passed/Discarded: 22/6/6, Total: 34, Passed Rate: 0.8235294117647058\n",
      "Name: Han Dongrui, Good/Passed/Discarded: 24/4/4, Total: 32, Passed Rate: 0.875\n",
      "Name: Cui Mingyu, Good/Passed/Discarded: 17/9/11, Total: 37, Passed Rate: 0.7027027027027027\n",
      "Name: eval_21, Good/Passed/Discarded: 17/9/13, Total: 39, Passed Rate: 0.6666666666666666\n",
      "Name: eval_22, Good/Passed/Discarded: 17/6/14, Total: 37, Passed Rate: 0.6216216216216216\n",
      "Name: eval_23, Good/Passed/Discarded: 13/18/8, Total: 39, Passed Rate: 0.7948717948717948\n",
      "Name: eval_24, Good/Passed/Discarded: 14/18/8, Total: 40, Passed Rate: 0.8\n",
      "Name: eval_25, Good/Passed/Discarded: 14/15/10, Total: 39, Passed Rate: 0.7435897435897436\n",
      "Name: eval_26, Good/Passed/Discarded: 15/13/5, Total: 33, Passed Rate: 0.8484848484848485\n",
      "Name: eval_27, Good/Passed/Discarded: 27/7/6, Total: 40, Passed Rate: 0.85\n",
      "Name: eval_28, Good/Passed/Discarded: 19/15/6, Total: 40, Passed Rate: 0.85\n",
      "Name: eval_29, Good/Passed/Discarded: 17/12/7, Total: 36, Passed Rate: 0.8055555555555556\n",
      "Name: eval_30, Good/Passed/Discarded: 7/20/11, Total: 38, Passed Rate: 0.7105263157894737\n",
      "Name: eval_31, Good/Passed/Discarded: 19/15/4, Total: 38, Passed Rate: 0.8947368421052632\n",
      "Name: eval_32, Good/Passed/Discarded: 14/14/8, Total: 36, Passed Rate: 0.7777777777777778\n",
      "Name: eval_33, Good/Passed/Discarded: 7/15/12, Total: 34, Passed Rate: 0.6470588235294118\n",
      "Name: eval_34, Good/Passed/Discarded: 9/16/10, Total: 35, Passed Rate: 0.7142857142857143\n",
      "Name: eval_35, Good/Passed/Discarded: 8/18/14, Total: 40, Passed Rate: 0.65\n",
      "Name: eval_36, Good/Passed/Discarded: 11/21/2, Total: 34, Passed Rate: 0.9411764705882353\n",
      "Total: 1379, Remain: 631, Border: 482, Failed: 266\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "names = [\"Wang Rui\", \"Peng Juewen\", \"Song Jiale\", \"Li Ang\", \"Ran Chen\", \"Li Miaomiao\", \"Deng Rui\", \"Yu Erxin\", \"Wang Hongru\", \"Du Yiming\", \n",
    "         \"Wang Zige\", \"Zhang Zhiwei\", \"Xue Boyang\", \"Wang Zezhong\", \"Kang Jiawen\", \"Zhu Qi\", \"Xu Hongling\", \"Hu Shujie\", \"Han Dongrui\", \"Cui Mingyu\",\n",
    "         \"eval_21\", \"eval_22\", \"eval_23\", \"eval_24\", \"eval_25\", \"eval_26\", \"eval_27\", \"eval_28\", \"eval_29\", \"eval_30\",\n",
    "         \"eval_31\", \"eval_32\", \"eval_33\", \"eval_34\", \"eval_35\", \"eval_36\"]\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v5-human/v2/{}.xlsx\"\n",
    "remain_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v5-human/remain.json\"\n",
    "border_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v5-human/border.json\"\n",
    "failed_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v5-human/failed.json\"\n",
    "\n",
    "remain_data_list, border_data_list, failed_data_list = [], [], []\n",
    "\n",
    "for name in names:\n",
    "    input_file = input_path.format(name)\n",
    "    \n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    dataset = df.to_dict(orient='records')\n",
    "\n",
    "    checked, passed = 0, 0\n",
    "    for data in dataset:\n",
    "        if isinstance(data[\"human_check\"], str):\n",
    "            failed_data_list.append(data)\n",
    "            pass\n",
    "        else:\n",
    "            if data[\"human_check\"] == 1:\n",
    "                checked += 1\n",
    "                if data[\"difficulty_eval\"] == 1:\n",
    "                    passed += 1\n",
    "                    remain_data_list.append(data)\n",
    "                else:\n",
    "                    border_data_list.append(data)\n",
    "            else:\n",
    "                failed_data_list.append(data)\n",
    "        \n",
    "    print(f\"Name: {name}, Good/Passed/Discarded: {passed}/{checked-passed}/{len(dataset)-checked}, Total: {len(dataset)}, Passed Rate: {checked/len(dataset)}\")\n",
    "\n",
    "print(f\"Total: {len(remain_data_list) + len(border_data_list) + len(failed_data_list)}, Remain: {len(remain_data_list)}, Border: {len(border_data_list)}, Failed: {len(failed_data_list)}\")\n",
    "\n",
    "with open(remain_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(remain_data_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(border_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(border_data_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(failed_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(failed_data_list, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631\n",
      "627\n",
      "482\n",
      "482\n",
      "132\n",
      "132 132\n",
      "295\n",
      "295 295\n",
      "324\n",
      "318 318\n",
      "358\n",
      "357 357\n",
      "Dataset: aime, Remove Count: 67, Contradict Count: 65, Data Size: 132\n",
      "Dataset: amc, Remove Count: 131, Contradict Count: 164, Data Size: 295\n",
      "Dataset: math, Remove Count: 154, Contradict Count: 164, Data Size: 318\n",
      "Dataset: minerva, Remove Count: 185, Contradict Count: 172, Data Size: 357\n",
      "Total Remove Count: 537, Total Contradict Count: 565, Total Count: 1102\n",
      "0.8014545454545454\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v5-human/{}.json\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/{}.json\"\n",
    "remain_type = [\"remain\", \"border\"]\n",
    "datasets = [\"aime\", \"amc\", \"math\", \"minerva\"]\n",
    "\n",
    "dataset_dict = {} \n",
    "for dataset in datasets:\n",
    "    dataset_dict[dataset] = []\n",
    "\n",
    "for remain in remain_type:\n",
    "    input_file = input_path.format(remain)\n",
    "    data_pool = read_json(input_file)\n",
    "\n",
    "    seen_ids = set()\n",
    "    unique_data = []\n",
    "    print(len(data_pool))\n",
    "    # data_sorted = sorted(dataset, key=lambda x: x['id'])\n",
    "    for item in data_pool:\n",
    "        if item['data_id'] not in seen_ids:\n",
    "            unique_data.append(item)\n",
    "            seen_ids.add(item['data_id'])\n",
    "    print(len(unique_data))\n",
    "    data_pool = unique_data\n",
    "\n",
    "    for data in data_pool:\n",
    "        data_source = data[\"data_id\"].split(\"_\")[0]\n",
    "        del data[\"solution\"]\n",
    "        del data[\"rewritten_condition\"]\n",
    "        del data[\"unsolvable_reason\"]\n",
    "        del data[\"human_check\"]\n",
    "        if \"Unnamed: 9\" in data:\n",
    "            del data[\"Unnamed: 9\"]\n",
    "        dataset_dict[data_source].append(data)\n",
    "\n",
    "for dataset in datasets:\n",
    "    seen_ids = set()\n",
    "    unique_data = []\n",
    "    data_pool = dataset_dict[dataset]\n",
    "    print(len(data_pool))\n",
    "    # data_sorted = sorted(dataset, key=lambda x: x['id'])\n",
    "    for item in data_pool:\n",
    "        if item['data_id'] not in seen_ids:\n",
    "            unique_data.append(item)\n",
    "            seen_ids.add(item['data_id'])\n",
    "    dataset_dict[dataset] = unique_data\n",
    "    print(len(unique_data), len(seen_ids))\n",
    "\n",
    "total_remove, total_contradict = 0, 0\n",
    "for dataset in datasets:\n",
    "    output_file = output_path.format(dataset)\n",
    "    remove, contradict = 0, 0\n",
    "    for data in dataset_dict[dataset]:\n",
    "        if \"remove\" in data[\"data_id\"]:\n",
    "            remove += 1\n",
    "        elif \"contradict\" in data[\"data_id\"]:\n",
    "            contradict += 1\n",
    "    assert remove + contradict == len(dataset_dict[dataset])\n",
    "    print(f\"Dataset: {dataset}, Remove Count: {remove}, Contradict Count: {contradict}, Data Size: {len(dataset_dict[dataset])}\")\n",
    "    total_remove += remove\n",
    "    total_contradict += contradict\n",
    "    write_json(output_file, dataset_dict[dataset])\n",
    "print(f\"Total Remove Count: {total_remove}, Total Contradict Count: {total_contradict}, Total Count: {total_remove + total_contradict}\")\n",
    "print(1102/1375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_r1_T0.0_std/unsol//aime.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_r1_T0.0_real/unsol//aime.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_v3_T0.0_std/unsol//aime.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_v3_T0.0_real/unsol//aime.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/o3-mini_T0.0_std/unsol//aime.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/gpt-4o_T0.0_std/unsol//aime.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/gpt-4o_T0.0_real/unsol//aime.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_r1_T0.0_std/unsol//amc.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_r1_T0.0_real/unsol//amc.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/o3-mini_T0.0_std/unsol//amc.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/gpt-4o_T0.0_std/unsol//amc.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/gpt-4o_T0.0_real/unsol//amc.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_r1_T0.0_std/unsol//math.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_r1_T0.0_real/unsol//math.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_v3_T0.0_std/unsol//math.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_v3_T0.0_real/unsol//math.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/o3-mini_T0.0_std/unsol//math.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/o3-mini_T0.0_real/unsol//math.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/gpt-4o_T0.0_std/unsol//math.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/gpt-4o_T0.0_real/unsol//math.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_r1_T0.0_std/unsol//minerva.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_r1_T0.0_real/unsol//minerva.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_v3_T0.0_std/unsol//minerva.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_v3_T0.0_real/unsol//minerva.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/o3-mini_T0.0_std/unsol//minerva.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/o3-mini_T0.0_real/unsol//minerva.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/gpt-4o_T0.0_std/unsol//minerva.json\n",
      "Error decoding JSON in file: /Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/gpt-4o_T0.0_real/unsol//minerva.json\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "datasets = [\"aime\", \"amc\", \"math\", \"minerva\"]\n",
    "models = [\"deepseek_r1\", \"deepseek_v3\", \"o3-mini\", \"gpt-4o\"]\n",
    "prompts = [\"std\", \"real\"]\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/{}_T0.0_{}/unsol//{}.json\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        for prompt in prompts:\n",
    "            input_file = input_path.format(model, prompt, dataset)\n",
    "            try:\n",
    "                data_pool = read_json(input_file)\n",
    "                json2jsonl(input_file, input_file)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON in file: {input_file}\")\n",
    "                continue\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {input_file}\")\n",
    "                continue\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['problem', 'solution', 'answer', 'year', 'aime_number', 'problem_number', 'difficulty'])\n",
      "Dataset: aime, Data Size: 975\n",
      "dict_keys(['problem', 'solution', 'answer', 'difficulty'])\n",
      "Dataset: amc, Data Size: 3264\n",
      "dict_keys(['problem', 'answer', 'difficulty', 'type'])\n",
      "Dataset: math, Data Size: 2298\n",
      "Total Data Size: 6537\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/train/{}.json\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/solve/train.json\"\n",
    "datasets = [\"aime\", \"amc\", \"math\"]\n",
    "\n",
    "train_data_list = []\n",
    "idx = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    input_file = input_path.format(dataset)\n",
    "    data_pool = read_json(input_file)\n",
    "    print(data_pool[0].keys())\n",
    "    # break\n",
    "    if dataset == \"math\":\n",
    "        data_list = []\n",
    "        for data in data_pool:\n",
    "            if data[\"difficulty\"] == 5:\n",
    "                data_list.append(data)\n",
    "                pass\n",
    "        # break\n",
    "        data_pool = data_list\n",
    "    \n",
    "    print(f\"Dataset: {dataset}, Data Size: {len(data_pool)}\")\n",
    "\n",
    "    for data in data_pool:\n",
    "        idx += 1\n",
    "        new_data = {\n",
    "            \"id\": idx,\n",
    "            \"data_source\": dataset,\n",
    "            \"question\": data[\"problem\"],\n",
    "            \"solution\": data[\"solution\"] if \"solution\" in data.keys() else None,\n",
    "            \"ground_truth\": data[\"answer\"]\n",
    "        }\n",
    "        train_data_list.append(new_data)\n",
    "        \n",
    "print(f\"Total Data Size: {len(train_data_list)}\")\n",
    "write_json(output_path, train_data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: math\n",
      "Remove Count: 873, Contradict Count: 66, Data Size: 939\n",
      "Dataset: amc\n",
      "Remove Count: 2952, Contradict Count: 233, Data Size: 3185\n",
      "Dataset: aime\n",
      "Remove Count: 920, Contradict Count: 41, Data Size: 961\n",
      "Total Count: 5085\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import json\n",
    "\n",
    "input_path = \"../data/unsol/v4-comp/train_unsolve.json\"\n",
    "total = 0\n",
    "data_pool = read_json(input_path)\n",
    "for dataset in [\"math\",\"amc\",\"aime\"]:\n",
    "    remove, contradict = 0, 0\n",
    "    for data in data_pool:\n",
    "        # print(data[\"generation\"])\n",
    "        if data[\"data_source\"] == dataset:\n",
    "            # print(data.keys())\n",
    "            if \"remove_question_1\" in data.keys():\n",
    "                remove += 1\n",
    "            elif \"contradict_question_1\" in data.keys():\n",
    "                contradict += 1\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(f\"Remove Count: {remove}, Contradict Count: {contradict}, Data Size: {remove + contradict}\")\n",
    "    total += remove + contradict\n",
    "print(f\"Total Count: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import json\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/distill-32b_T0.0_std/solve/amc.json\"\n",
    "\n",
    "data_pool = read_jsonl(input_path)\n",
    "for data in data_pool:\n",
    "    # print(data[\"generation\"])\n",
    "    generation = data[\"generation\"][0]\n",
    "    if isinstance(generation, str):\n",
    "        generation = json.loads(generation)\n",
    "        # print(generation.keys())\n",
    "        response = {\n",
    "            \"reasoning\": generation[\"choices\"][0][\"message\"][\"reasoning_content\"],\n",
    "            \"answer\": generation[\"choices\"][0][\"message\"][\"content\"]\n",
    "        }\n",
    "        data[\"generation\"] = [response]\n",
    "    # print(data)\n",
    "    # break\n",
    "\n",
    "print(len(data_pool))\n",
    "write_jsonl(input_path, data_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: aime, Task: solve, Model: qwen-1.5b, Reliability: std, Data Size: 60\n",
      "Dataset: aime, Task: solve, Model: qwen-1.5b, Reliability: std, Data Size: 30\n",
      "Dataset: aime, Task: solve, Model: qwen-1.5b, Reliability: real, Data Size: 60\n",
      "Dataset: aime, Task: solve, Model: qwen-1.5b, Reliability: real, Data Size: 30\n",
      "Dataset: aime, Task: solve, Model: qwen-7b, Reliability: std, Data Size: 60\n",
      "Dataset: aime, Task: solve, Model: qwen-7b, Reliability: std, Data Size: 30\n",
      "Dataset: aime, Task: solve, Model: qwen-7b, Reliability: real, Data Size: 60\n",
      "Dataset: aime, Task: solve, Model: qwen-7b, Reliability: real, Data Size: 30\n",
      "Dataset: aime, Task: solve, Model: distill-1.5b, Reliability: std, Data Size: 60\n",
      "Dataset: aime, Task: solve, Model: distill-1.5b, Reliability: std, Data Size: 30\n",
      "Dataset: aime, Task: solve, Model: distill-1.5b, Reliability: real, Data Size: 60\n",
      "Dataset: aime, Task: solve, Model: distill-1.5b, Reliability: real, Data Size: 30\n",
      "Dataset: aime, Task: solve, Model: distill-7b, Reliability: std, Data Size: 60\n",
      "Dataset: aime, Task: solve, Model: distill-7b, Reliability: std, Data Size: 30\n",
      "Dataset: aime, Task: solve, Model: distill-7b, Reliability: real, Data Size: 60\n",
      "Dataset: aime, Task: solve, Model: distill-7b, Reliability: real, Data Size: 30\n",
      "Dataset: aime, Task: unsol, Model: qwen-1.5b, Reliability: std, Data Size: 220\n",
      "Dataset: aime, Task: unsol, Model: qwen-1.5b, Reliability: std, Data Size: 110\n",
      "Dataset: aime, Task: unsol, Model: qwen-1.5b, Reliability: real, Data Size: 220\n",
      "Dataset: aime, Task: unsol, Model: qwen-1.5b, Reliability: real, Data Size: 110\n",
      "Dataset: aime, Task: unsol, Model: qwen-7b, Reliability: std, Data Size: 220\n",
      "Dataset: aime, Task: unsol, Model: qwen-7b, Reliability: std, Data Size: 110\n",
      "Dataset: aime, Task: unsol, Model: qwen-7b, Reliability: real, Data Size: 220\n",
      "Dataset: aime, Task: unsol, Model: qwen-7b, Reliability: real, Data Size: 110\n",
      "Dataset: aime, Task: unsol, Model: distill-1.5b, Reliability: std, Data Size: 220\n",
      "Dataset: aime, Task: unsol, Model: distill-1.5b, Reliability: std, Data Size: 110\n",
      "Dataset: aime, Task: unsol, Model: distill-1.5b, Reliability: real, Data Size: 220\n",
      "Dataset: aime, Task: unsol, Model: distill-1.5b, Reliability: real, Data Size: 110\n",
      "Dataset: aime, Task: unsol, Model: distill-7b, Reliability: std, Data Size: 220\n",
      "Dataset: aime, Task: unsol, Model: distill-7b, Reliability: std, Data Size: 110\n",
      "Dataset: aime, Task: unsol, Model: distill-7b, Reliability: real, Data Size: 220\n",
      "Dataset: aime, Task: unsol, Model: distill-7b, Reliability: real, Data Size: 110\n",
      "Dataset: amc, Task: solve, Model: qwen-1.5b, Reliability: std, Data Size: 166\n",
      "Dataset: amc, Task: solve, Model: qwen-1.5b, Reliability: std, Data Size: 83\n",
      "Dataset: amc, Task: solve, Model: qwen-1.5b, Reliability: real, Data Size: 166\n",
      "Dataset: amc, Task: solve, Model: qwen-1.5b, Reliability: real, Data Size: 83\n",
      "Dataset: amc, Task: solve, Model: qwen-7b, Reliability: std, Data Size: 166\n",
      "Dataset: amc, Task: solve, Model: qwen-7b, Reliability: std, Data Size: 83\n",
      "Dataset: amc, Task: solve, Model: qwen-7b, Reliability: real, Data Size: 166\n",
      "Dataset: amc, Task: solve, Model: qwen-7b, Reliability: real, Data Size: 83\n",
      "Dataset: amc, Task: solve, Model: distill-1.5b, Reliability: std, Data Size: 166\n",
      "Dataset: amc, Task: solve, Model: distill-1.5b, Reliability: std, Data Size: 83\n",
      "Dataset: amc, Task: solve, Model: distill-1.5b, Reliability: real, Data Size: 166\n",
      "Dataset: amc, Task: solve, Model: distill-1.5b, Reliability: real, Data Size: 83\n",
      "Dataset: amc, Task: solve, Model: distill-7b, Reliability: std, Data Size: 166\n",
      "Dataset: amc, Task: solve, Model: distill-7b, Reliability: std, Data Size: 83\n",
      "Dataset: amc, Task: solve, Model: distill-7b, Reliability: real, Data Size: 166\n",
      "Dataset: amc, Task: solve, Model: distill-7b, Reliability: real, Data Size: 83\n",
      "Dataset: amc, Task: unsol, Model: qwen-1.5b, Reliability: std, Data Size: 252\n",
      "Dataset: amc, Task: unsol, Model: qwen-1.5b, Reliability: std, Data Size: 126\n",
      "Dataset: amc, Task: unsol, Model: qwen-1.5b, Reliability: real, Data Size: 252\n",
      "Dataset: amc, Task: unsol, Model: qwen-1.5b, Reliability: real, Data Size: 126\n",
      "Dataset: amc, Task: unsol, Model: qwen-7b, Reliability: std, Data Size: 252\n",
      "Dataset: amc, Task: unsol, Model: qwen-7b, Reliability: std, Data Size: 126\n",
      "Dataset: amc, Task: unsol, Model: qwen-7b, Reliability: real, Data Size: 252\n",
      "Dataset: amc, Task: unsol, Model: qwen-7b, Reliability: real, Data Size: 126\n",
      "Dataset: amc, Task: unsol, Model: distill-1.5b, Reliability: std, Data Size: 252\n",
      "Dataset: amc, Task: unsol, Model: distill-1.5b, Reliability: std, Data Size: 126\n",
      "Dataset: amc, Task: unsol, Model: distill-1.5b, Reliability: real, Data Size: 252\n",
      "Dataset: amc, Task: unsol, Model: distill-1.5b, Reliability: real, Data Size: 126\n",
      "Dataset: amc, Task: unsol, Model: distill-7b, Reliability: std, Data Size: 252\n",
      "Dataset: amc, Task: unsol, Model: distill-7b, Reliability: std, Data Size: 126\n",
      "Dataset: amc, Task: unsol, Model: distill-7b, Reliability: real, Data Size: 252\n",
      "Dataset: amc, Task: unsol, Model: distill-7b, Reliability: real, Data Size: 126\n",
      "Dataset: math, Task: solve, Model: qwen-1.5b, Reliability: std, Data Size: 200\n",
      "Dataset: math, Task: solve, Model: qwen-1.5b, Reliability: std, Data Size: 100\n",
      "Dataset: math, Task: solve, Model: qwen-1.5b, Reliability: real, Data Size: 200\n",
      "Dataset: math, Task: solve, Model: qwen-1.5b, Reliability: real, Data Size: 100\n",
      "Dataset: math, Task: solve, Model: qwen-7b, Reliability: std, Data Size: 200\n",
      "Dataset: math, Task: solve, Model: qwen-7b, Reliability: std, Data Size: 100\n",
      "Dataset: math, Task: solve, Model: qwen-7b, Reliability: real, Data Size: 200\n",
      "Dataset: math, Task: solve, Model: qwen-7b, Reliability: real, Data Size: 100\n",
      "Dataset: math, Task: solve, Model: distill-1.5b, Reliability: std, Data Size: 200\n",
      "Dataset: math, Task: solve, Model: distill-1.5b, Reliability: std, Data Size: 100\n",
      "Dataset: math, Task: solve, Model: distill-1.5b, Reliability: real, Data Size: 200\n",
      "Dataset: math, Task: solve, Model: distill-1.5b, Reliability: real, Data Size: 100\n",
      "Dataset: math, Task: solve, Model: distill-7b, Reliability: std, Data Size: 200\n",
      "Dataset: math, Task: solve, Model: distill-7b, Reliability: std, Data Size: 100\n",
      "Dataset: math, Task: solve, Model: distill-7b, Reliability: real, Data Size: 200\n",
      "Dataset: math, Task: solve, Model: distill-7b, Reliability: real, Data Size: 100\n",
      "Dataset: math, Task: unsol, Model: qwen-1.5b, Reliability: std, Data Size: 124\n",
      "Dataset: math, Task: unsol, Model: qwen-1.5b, Reliability: std, Data Size: 0\n",
      "Dataset: math, Task: unsol, Model: qwen-1.5b, Reliability: real, Data Size: 124\n",
      "Dataset: math, Task: unsol, Model: qwen-1.5b, Reliability: real, Data Size: 0\n",
      "Dataset: math, Task: unsol, Model: qwen-7b, Reliability: std, Data Size: 124\n",
      "Dataset: math, Task: unsol, Model: qwen-7b, Reliability: std, Data Size: 0\n",
      "Dataset: math, Task: unsol, Model: qwen-7b, Reliability: real, Data Size: 124\n",
      "Dataset: math, Task: unsol, Model: qwen-7b, Reliability: real, Data Size: 0\n",
      "Dataset: math, Task: unsol, Model: distill-1.5b, Reliability: std, Data Size: 124\n",
      "Dataset: math, Task: unsol, Model: distill-1.5b, Reliability: std, Data Size: 0\n",
      "Dataset: math, Task: unsol, Model: distill-1.5b, Reliability: real, Data Size: 124\n",
      "Dataset: math, Task: unsol, Model: distill-1.5b, Reliability: real, Data Size: 0\n",
      "Dataset: math, Task: unsol, Model: distill-7b, Reliability: std, Data Size: 124\n",
      "Dataset: math, Task: unsol, Model: distill-7b, Reliability: std, Data Size: 0\n",
      "Dataset: math, Task: unsol, Model: distill-7b, Reliability: real, Data Size: 124\n",
      "Dataset: math, Task: unsol, Model: distill-7b, Reliability: real, Data Size: 0\n",
      "Dataset: minerva, Task: solve, Model: qwen-1.5b, Reliability: std, Data Size: 200\n",
      "Dataset: minerva, Task: solve, Model: qwen-1.5b, Reliability: std, Data Size: 100\n",
      "Dataset: minerva, Task: solve, Model: qwen-1.5b, Reliability: real, Data Size: 200\n",
      "Dataset: minerva, Task: solve, Model: qwen-1.5b, Reliability: real, Data Size: 100\n",
      "Dataset: minerva, Task: solve, Model: qwen-7b, Reliability: std, Data Size: 200\n",
      "Dataset: minerva, Task: solve, Model: qwen-7b, Reliability: std, Data Size: 100\n",
      "Dataset: minerva, Task: solve, Model: qwen-7b, Reliability: real, Data Size: 200\n",
      "Dataset: minerva, Task: solve, Model: qwen-7b, Reliability: real, Data Size: 100\n",
      "Dataset: minerva, Task: solve, Model: distill-1.5b, Reliability: std, Data Size: 200\n",
      "Dataset: minerva, Task: solve, Model: distill-1.5b, Reliability: std, Data Size: 100\n",
      "Dataset: minerva, Task: solve, Model: distill-1.5b, Reliability: real, Data Size: 200\n",
      "Dataset: minerva, Task: solve, Model: distill-1.5b, Reliability: real, Data Size: 100\n",
      "Dataset: minerva, Task: solve, Model: distill-7b, Reliability: std, Data Size: 200\n",
      "Dataset: minerva, Task: solve, Model: distill-7b, Reliability: std, Data Size: 100\n",
      "Dataset: minerva, Task: solve, Model: distill-7b, Reliability: real, Data Size: 200\n",
      "Dataset: minerva, Task: solve, Model: distill-7b, Reliability: real, Data Size: 100\n",
      "Dataset: minerva, Task: unsol, Model: qwen-1.5b, Reliability: std, Data Size: 294\n",
      "Dataset: minerva, Task: unsol, Model: qwen-1.5b, Reliability: std, Data Size: 147\n",
      "Dataset: minerva, Task: unsol, Model: qwen-1.5b, Reliability: real, Data Size: 294\n",
      "Dataset: minerva, Task: unsol, Model: qwen-1.5b, Reliability: real, Data Size: 147\n",
      "Dataset: minerva, Task: unsol, Model: qwen-7b, Reliability: std, Data Size: 294\n",
      "Dataset: minerva, Task: unsol, Model: qwen-7b, Reliability: std, Data Size: 147\n",
      "Dataset: minerva, Task: unsol, Model: qwen-7b, Reliability: real, Data Size: 294\n",
      "Dataset: minerva, Task: unsol, Model: qwen-7b, Reliability: real, Data Size: 147\n",
      "Dataset: minerva, Task: unsol, Model: distill-1.5b, Reliability: std, Data Size: 294\n",
      "Dataset: minerva, Task: unsol, Model: distill-1.5b, Reliability: std, Data Size: 147\n",
      "Dataset: minerva, Task: unsol, Model: distill-1.5b, Reliability: real, Data Size: 294\n",
      "Dataset: minerva, Task: unsol, Model: distill-1.5b, Reliability: real, Data Size: 147\n",
      "Dataset: minerva, Task: unsol, Model: distill-7b, Reliability: std, Data Size: 294\n",
      "Dataset: minerva, Task: unsol, Model: distill-7b, Reliability: std, Data Size: 147\n",
      "Dataset: minerva, Task: unsol, Model: distill-7b, Reliability: real, Data Size: 294\n",
      "Dataset: minerva, Task: unsol, Model: distill-7b, Reliability: real, Data Size: 147\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"aime\", \"amc\", \"math\", \"minerva\"]\n",
    "tasks = [\"solve\", \"unsol\"]\n",
    "reliability = [\"std\", \"real\"]\n",
    "models = [\"qwen-1.5b\", \"qwen-7b\", \"distill-1.5b\", \"distill-7b\"]\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/{}_T0.0_{}/{}/{}.json\"\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for task in tasks:\n",
    "        for model in models:\n",
    "            for rel in reliability:\n",
    "                input_file = input_path.format(model, rel, task, dataset)\n",
    "                data_pool = read_jsonl(input_file)\n",
    "                print(f\"Dataset: {dataset}, Task: {task}, Model: {model}, Reliability: {rel}, Data Size: {len(data_pool)}\")\n",
    "                data_list = []\n",
    "                for data in data_pool:\n",
    "                    if isinstance(data[\"generation\"], list):\n",
    "                        data_list.append(data)\n",
    "                print(f\"Dataset: {dataset}, Task: {task}, Model: {model}, Reliability: {rel}, Data Size: {len(data_list)}\")\n",
    "                write_jsonl(input_file, data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563\n",
      "1563\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/train_analysis.json\"\n",
    "\n",
    "seen_ids = set()\n",
    "unique_data = []\n",
    "dataset = read_jsonl(input_file)\n",
    "print(len(dataset))\n",
    "for item in dataset:\n",
    "    if item['id'] not in seen_ids:\n",
    "        unique_data.append(item)\n",
    "        seen_ids.add(item['id'])\n",
    "\n",
    "print(len(unique_data))\n",
    "write_jsonl(input_file, unique_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6537\n",
      "6537\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/train_analysis.json\"\n",
    "\n",
    "seen_ids = set()\n",
    "unique_data = []\n",
    "dataset = read_jsonl(input_file)\n",
    "print(len(dataset))\n",
    "data_sorted = sorted(dataset, key=lambda x: x['id'])\n",
    "\n",
    "print(len(data_sorted))\n",
    "write_json(input_file, data_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5194\n",
      "5194\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/train_unsolve.json\"\n",
    "\n",
    "seen_ids = set()\n",
    "unique_data = []\n",
    "dataset = read_jsonl(input_file)\n",
    "print(len(dataset))\n",
    "# data_sorted = sorted(dataset, key=lambda x: x['id'])\n",
    "for item in dataset:\n",
    "    if item['id'] not in seen_ids:\n",
    "        unique_data.append(item)\n",
    "        seen_ids.add(item['id'])\n",
    "\n",
    "print(len(unique_data))\n",
    "write_jsonl(input_file, unique_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5194\n",
      "5194\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/train_unsolve.json\"\n",
    "\n",
    "seen_ids = set()\n",
    "unique_data = []\n",
    "dataset = read_jsonl(input_file)\n",
    "print(len(dataset))\n",
    "data_sorted = sorted(dataset, key=lambda x: x['id'])\n",
    "\n",
    "print(len(data_sorted))\n",
    "write_json(input_file, data_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train, New Data Count: 7740\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/v4-comp/{}_check.json\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/train.json\"\n",
    "datasets = [\"train\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    input_file = input_path.format(dataset)\n",
    "    try:\n",
    "        data_pool = read_json(input_file)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON in file: {input_file}\")\n",
    "        continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {input_file}\")\n",
    "        continue\n",
    "    \n",
    "    new_data_list = []\n",
    "    data_id_dict = []\n",
    "    for idx, data in enumerate(data_pool):\n",
    "        if dataset in [\"math\", \"minerva\"]:\n",
    "            data_id_dict.append({\"idx\": data[\"id\"]})\n",
    "        for unsolve_type in UNS_TYPE:\n",
    "            count = 0\n",
    "            for key in data.keys():\n",
    "                if key.startswith(unsolve_type + \"_question_\"):\n",
    "                    new_data = {\n",
    "                        \"data_id\": data[\"data_source\"] + \"_\" + str(idx) + \"_\"  + unsolve_type + \"_\" + str(count+1),\n",
    "                        # \"data_source\": data[\"data_source\"],\n",
    "                        \"question\": data[\"question\"],\n",
    "                        \"ground_truth\": data[\"ground_truth\"],\n",
    "                        # \"solution\": data[\"solution\"] if data[\"data_source\"] in [\"math\"] else None,\n",
    "                        # \"unsolve_id\": unsolve_type + \"_\" + str(count+1),\n",
    "                        \"rewritten_question\": data[unsolve_type + \"_question_\" + str(count+1)][unsolve_type + \"_question\"]\n",
    "                    }\n",
    "\n",
    "                    # if dataset in [\"math\", \"aime\"]:\n",
    "                    #     new_data[\"solution\"] = data[\"solution\"]\n",
    "                    count += 1\n",
    "                    new_data_list.append(new_data)\n",
    "\n",
    "    print(f\"Dataset: {dataset}, New Data Count: {len(new_data_list)}\")\n",
    "    # Save the new data to a JSON file\n",
    "    output_file = output_path.format(dataset)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as fw:\n",
    "        json.dump(new_data_list, fw, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Input Size: 132\n",
      "Dataset: aime, Data Size: 132, Processed Data Size: 132, Remaining Data Size: 0\n",
      "Current Input Size: 295\n",
      "Dataset: amc, Data Size: 295, Processed Data Size: 295, Remaining Data Size: 0\n",
      "Current Input Size: 318\n",
      "Dataset: math, Data Size: 318, Processed Data Size: 318, Remaining Data Size: 0\n",
      "Current Input Size: 357\n",
      "Dataset: minerva, Data Size: 357, Processed Data Size: 357, Remaining Data Size: 0\n",
      "All Data Size: 1102, Prompt: std, Processed Data Size: 1102, Remaining Data Size: 0\n",
      "\n",
      "Current Input Size: 132\n",
      "Dataset: aime, Data Size: 132, Processed Data Size: 132, Remaining Data Size: 0\n",
      "Current Input Size: 295\n",
      "Dataset: amc, Data Size: 295, Processed Data Size: 295, Remaining Data Size: 0\n",
      "Current Input Size: 318\n",
      "Dataset: math, Data Size: 318, Processed Data Size: 318, Remaining Data Size: 0\n",
      "Current Input Size: 357\n",
      "Dataset: minerva, Data Size: 357, Processed Data Size: 357, Remaining Data Size: 0\n",
      "All Data Size: 1102, Prompt: real, Processed Data Size: 1102, Remaining Data Size: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/distill-32b_T0.0_{}/unsol/{}.json\"\n",
    "data_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/{}.json\"\n",
    "datasets = [\"aime\", \"amc\", \"math\", \"minerva\"]\n",
    "prompts = [\"std\", \"real\"]\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    all_data, processed_data = 0, 0\n",
    "    for dataset in datasets:\n",
    "        seen_ids = set()\n",
    "        unique_data = []\n",
    "        # print(dataset)\n",
    "        input_file = input_path.format(prompt, dataset)\n",
    "        data_file = data_path.format(dataset)\n",
    "        data_pool = read_jsonl(input_file)\n",
    "        keys = [data[\"data_id\"] for data in read_json(data_file)]\n",
    "        print(f\"Current Input Size: {len(data_pool)}\")\n",
    "        # data_sorted = sorted(dataset, key=lambda x: x['id'])\n",
    "        for item in data_pool:\n",
    "            if item['data_id'] not in seen_ids:\n",
    "                unique_data.append(item)\n",
    "                seen_ids.add(item['data_id'])\n",
    "\n",
    "        # if len(unique_data) == len(read_json(data_file)):\n",
    "        #     # print(f\"All data are unique, no need to process.\")\n",
    "        #     continue\n",
    "        print(f\"Dataset: {dataset}, Data Size: {len(read_json(data_file))}, Processed Data Size: {len(unique_data)}, Remaining Data Size: {len(read_json(data_file)) - len(unique_data)}\")\n",
    "        write_jsonl(input_file, unique_data)\n",
    "        all_data += len(read_json(data_file))\n",
    "        processed_data += len(unique_data)\n",
    "    print(f\"All Data Size: {all_data}, Prompt: {prompt}, Processed Data Size: {processed_data}, Remaining Data Size: {all_data - processed_data}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "input_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/./exp/deepseek_r1_T0.0_real/solve/train.json\"\n",
    "\n",
    "seen_ids = set()\n",
    "unique_data = []\n",
    "dataset = read_jsonl(input_file)\n",
    "print(len(dataset))\n",
    "# data_sorted = sorted(dataset, key=lambda x: x['id'])\n",
    "for item in dataset:\n",
    "    if item['data_id'] not in seen_ids:\n",
    "        unique_data.append(item)\n",
    "        seen_ids.add(item['data_id'])\n",
    "\n",
    "print(len(unique_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: aime, Prompt: std, Task: solve, Avg Length: 3456.0\n",
      "Dataset: aime, Prompt: std, Task: unsol, Avg Length: 8467.8\n",
      "Dataset: aime, Prompt: real, Task: solve, Avg Length: 3890.6\n",
      "Dataset: aime, Prompt: real, Task: unsol, Avg Length: 5214.4\n",
      "Dataset: amc, Prompt: std, Task: solve, Avg Length: 2244.4444444444443\n",
      "Dataset: amc, Prompt: std, Task: unsol, Avg Length: 5599.285714285715\n",
      "Dataset: amc, Prompt: real, Task: solve, Avg Length: 2908.4\n",
      "Dataset: amc, Prompt: real, Task: unsol, Avg Length: 5673.7\n",
      "Dataset: math, Prompt: std, Task: solve, Avg Length: 1247.3\n",
      "Dataset: math, Prompt: std, Task: unsol, Avg Length: 6184.0\n",
      "Dataset: math, Prompt: real, Task: solve, Avg Length: 817.5\n",
      "Dataset: math, Prompt: real, Task: unsol, Avg Length: 5219.2\n",
      "Dataset: minerva, Prompt: std, Task: solve, Avg Length: 591.2\n",
      "Dataset: minerva, Prompt: std, Task: unsol, Avg Length: 1902.2\n",
      "Dataset: minerva, Prompt: real, Task: solve, Avg Length: 530.3\n",
      "Dataset: minerva, Prompt: real, Task: unsol, Avg Length: 1918.9\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from utils import *\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/gpt-4o-mini_T0.0_{}/{}/{}.json\"\n",
    "datasets = [\"aime\", \"amc\", \"math\", \"minerva\"]\n",
    "prompts = [\"std\", \"real\"]\n",
    "tasks = [\"solve\", \"unsol\"]\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(tokenizer_dict[\"o3-mini\"])\n",
    "\n",
    "for dataset in datasets:\n",
    "    for prompt in prompts:\n",
    "        for task in tasks:\n",
    "            input_file = input_path.format(prompt, task, dataset)\n",
    "            if not os.path.exists(input_file):\n",
    "                continue\n",
    "            data_pool = read_jsonl(input_file)\n",
    "            length = 0\n",
    "            for data in data_pool:\n",
    "                length += data[\"generation\"][0][\"reasoning\"] + len(encoding.encode(data[\"generation\"][0][\"answer\"]))\n",
    "                # length_2 += len(encoding.encode(data[\"generation\"][0][\"answer\"]))\n",
    "\n",
    "            avg_length = length / len(data_pool)\n",
    "            # avg_length_2 = length_2 / len(data_pool)\n",
    "            print(f\"Dataset: {dataset}, Prompt: {prompt}, Task: {task}, Avg Length: {avg_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6537\n",
      "6537\n",
      "Total Data Size: 6537, Success Data Size: 5344, Failed Data Size: 1193\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/./exp/deepseek_r1_T0.0_std/solve/train.json\"\n",
    "\n",
    "seen_ids = set()\n",
    "unique_data = []\n",
    "dataset = read_jsonl(input_file)\n",
    "print(len(dataset))\n",
    "# data_sorted = sorted(dataset, key=lambda x: x['id'])\n",
    "for item in dataset:\n",
    "    if item['id'] not in seen_ids:\n",
    "        unique_data.append(item)\n",
    "        seen_ids.add(item['id'])\n",
    "\n",
    "print(len(unique_data))\n",
    "write_jsonl(input_file, unique_data)\n",
    "\n",
    "from utils import *\n",
    "from metrics.rewards.math_reward import deepscaler_reward_fn, realmath_reward_fn\n",
    "\n",
    "input_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/./exp/deepseek_r1_T0.0_std/solve/train.json\"\n",
    "output_file_1 = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/train/solve_success.json\"\n",
    "output_file_2 =  \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/solve/train_distill.json\"\n",
    "\n",
    "dataset = read_jsonl(input_file)\n",
    "success_data = []\n",
    "failed_data = []\n",
    "for data in dataset:\n",
    "    ground_truth = data[\"ground_truth\"]\n",
    "    answer = data[\"generation\"][0][\"answer\"]\n",
    "    _, judge, _ = realmath_reward_fn(answer, ground_truth, task=\"solve\")\n",
    "    if judge == \"correct\":\n",
    "        success_data.append(data)\n",
    "    else:\n",
    "        failed_data.append(data)\n",
    "\n",
    "print(f\"Total Data Size: {len(dataset)}, Success Data Size: {len(success_data)}, Failed Data Size: {len(failed_data)}\")\n",
    "write_jsonl(output_file_1, success_data)\n",
    "write_jsonl(output_file_2, failed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740\n",
      "7740\n",
      "Data 7119 has no generation.\n",
      "Data 7120 has no generation.\n",
      "Data 7121 has no generation.\n",
      "Data 7122 has no generation.\n",
      "Data 7123 has no generation.\n",
      "Total Data Size: 7740, Success Data Size: 4191, Unknown Data Size: 57, Failed Data Size: 3492\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/./exp/deepseek_r1_T0.0_real/unsol/train.json\"\n",
    "\n",
    "seen_ids = set()\n",
    "unique_data = []\n",
    "dataset = read_jsonl(input_file)\n",
    "print(len(dataset))\n",
    "# data_sorted = sorted(dataset, key=lambda x: x['id'])\n",
    "for item in dataset:\n",
    "    if item['data_id'] not in seen_ids:\n",
    "        unique_data.append(item)\n",
    "        seen_ids.add(item['data_id'])\n",
    "\n",
    "print(len(unique_data))\n",
    "write_jsonl(input_file, unique_data)\n",
    "\n",
    "from utils import *\n",
    "from metrics.rewards.math_reward import realmath_reward_fn\n",
    "\n",
    "input_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/./exp/deepseek_r1_T0.0_real/unsol/train.json\"\n",
    "output_file = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/train/unsol_success.json\"\n",
    "output_file_1 = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/train/unsol_refuse.json\"\n",
    "output_file_2 =  \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/train_distill.json\"\n",
    "\n",
    "dataset = read_jsonl(input_file)\n",
    "success_data, unknown_data, failed_data = [], [], []\n",
    "for idx, data in enumerate(dataset):\n",
    "    ground_truth = data[\"ground_truth\"]\n",
    "    if len(data[\"generation\"]) == 0:\n",
    "        print(f\"Data {idx} has no generation.\")\n",
    "        failed_data.append(data)\n",
    "        continue\n",
    "    answer = data[\"generation\"][0][\"answer\"]\n",
    "    _, judge, _ = realmath_reward_fn(answer, ground_truth, task=\"unsol\")\n",
    "    if judge == \"unsolvable\":\n",
    "        success_data.append(data)\n",
    "    elif judge == \"unknown\":\n",
    "        unknown_data.append(data)\n",
    "    else:\n",
    "        failed_data.append(data)\n",
    "    # dump_jsonl(data, output_file, append=True)\n",
    "print(f\"Total Data Size: {len(dataset)}, Success Data Size: {len(success_data)}, Unknown Data Size: {len(unknown_data)}, Failed Data Size: {len(failed_data)}\")\n",
    "write_jsonl(output_file, success_data)\n",
    "write_jsonl(output_file_1, unknown_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "Dataset: aime, Remove Count: 24, Remove Count: 43, Contradict Count: 22, Contradict Count: 43\n",
      "Total Remove: 67\n",
      "Total Contradict: 65\n",
      "295\n",
      "Dataset: amc, Remove Count: 47, Remove Count: 84, Contradict Count: 60, Contradict Count: 104\n",
      "Total Remove: 131\n",
      "Total Contradict: 164\n",
      "318\n",
      "Dataset: math, Remove Count: 77, Remove Count: 77, Contradict Count: 75, Contradict Count: 89\n",
      "Total Remove: 154\n",
      "Total Contradict: 164\n",
      "357\n",
      "Dataset: minerva, Remove Count: 87, Remove Count: 98, Contradict Count: 76, Contradict Count: 96\n",
      "Total Remove: 185\n",
      "Total Contradict: 172\n",
      "Total Remove: 537\n",
      "Total Contradict: 565\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "datasets = [\"aime\", \"amc\", \"math\", \"minerva\"]\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/unsol/{}.json\"\n",
    "\n",
    "cont, remove = 0, 0\n",
    "for dataset in datasets:\n",
    "    input_file = input_path.format(dataset)\n",
    "    data_pool = read_json(input_file)\n",
    "    print(len(data_pool))\n",
    "    remove_diff0, remove_diff1 = 0, 0\n",
    "    cont_diff0, cont_diff1 = 0, 0\n",
    "    for item in data_pool:\n",
    "        # print(item.keys())\n",
    "        if \"remove\" in item['data_id']:\n",
    "            if item[\"difficulty_eval\"] == 0:\n",
    "                remove_diff0 += 1\n",
    "            elif item[\"difficulty_eval\"] == 1:\n",
    "                remove_diff1 += 1\n",
    "            else:\n",
    "                print(f\"Error: {item['data_id']}\")\n",
    "                item[\"difficulty_eval\"] = 1\n",
    "                continue\n",
    "        elif \"contradict\" in item['data_id']:\n",
    "            if item[\"difficulty_eval\"] == 0:\n",
    "                cont_diff0 += 1\n",
    "            elif item[\"difficulty_eval\"] == 1:\n",
    "                cont_diff1 += 1\n",
    "            else:\n",
    "                print(f\"Error: {item['data_id']}\")\n",
    "                item[\"difficulty_eval\"] = 1\n",
    "                continue\n",
    "    \n",
    "    print(f\"Dataset: {dataset}, Remove Count: {remove_diff0}, Remove Count: {remove_diff1}, Contradict Count: {cont_diff0}, Contradict Count: {cont_diff1}\")\n",
    "    print(f\"Total Remove: {remove_diff0 + remove_diff1}\")\n",
    "    print(f\"Total Contradict: {cont_diff0 + cont_diff1}\")\n",
    "    cont += cont_diff0 + cont_diff1\n",
    "    remove += remove_diff0 + remove_diff1\n",
    "    # write_json(input_file, data_pool)\n",
    "print(f\"Total Remove: {remove}\")\n",
    "print(f\"Total Contradict: {cont}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: solve, Data Size: 5344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17434 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: unsol, Data Size: 3928\n",
      "Average Length: 3371.5076574633304\n",
      "Total Data Size: 9272\n",
      "Task: solve, Data Size: 5344\n",
      "Task: unsol, Data Size: 3928\n",
      "Average Length: 298.0263157894737\n",
      "Total Data Size: 9272\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/train/{}_success.json\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/train/train_{}.json\"\n",
    "prompt_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/prompt/math_instruction.json\"\n",
    "tasks = [\"solve\", \"unsol\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\")\n",
    "\n",
    "prompt = load_json(prompt_path)\n",
    "# print(prompt)\n",
    "\n",
    "for student in [\"reason\", \"instruct\"]:\n",
    "    new_dataset = []\n",
    "    length = 0\n",
    "    for task in tasks:\n",
    "        input_file = input_path.format(task)\n",
    "        data_pool = read_jsonl(input_file)\n",
    "        print(f\"Task: {task}, Data Size: {len(data_pool)}\")\n",
    "        for data in data_pool:\n",
    "            reasoning = data[\"generation\"][0][\"reasoning\"]\n",
    "            answer = data[\"generation\"][0][\"answer\"]\n",
    "            if task == \"solve\":\n",
    "                response = reasoning + answer if student == \"reason\" else answer\n",
    "                tokens = tokenizer.encode(response)\n",
    "                new_data = {\n",
    "                    \"id\": data[\"id\"],\n",
    "                    \"instruction\": data[\"question\"],\n",
    "                    \"input\": prompt[\"real\"],\n",
    "                    \"output\": response\n",
    "                }\n",
    "            else:\n",
    "                response = reasoning + answer if student == \"reason\" else answer\n",
    "                tokens = tokenizer.encode(response)\n",
    "                try:\n",
    "                    # print(data[\"data_id\"])\n",
    "                    new_data = {\n",
    "                        \"id\": data[\"data_id\"],\n",
    "                        \"instruction\": data[\"rewritten_question\"],\n",
    "                        \"input\": prompt[\"real\"],\n",
    "                        \"output\": response\n",
    "                    }\n",
    "                except KeyError:\n",
    "                    print(f\"KeyError in data: {data}\")\n",
    "                    continue\n",
    "            length += len(tokens)\n",
    "            new_dataset.append(new_data)\n",
    "    print(f\"Average Length: {length / len(new_dataset)}\")\n",
    "    print(f\"Total Data Size: {len(new_dataset)}\")\n",
    "    output_file = output_path.format(student)\n",
    "    write_jsonl(output_file, new_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: solve, Data Size: 481\n",
      "Task: unsol, Data Size: 787\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import random\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/exp/deepseek_r1_T0.0_ref/{}/distill.json\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/train/{}_refuse.json\"\n",
    "from metrics.rewards.math_reward import realmath_reward_fn\n",
    "\n",
    "prompt_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/prompt/math_instruction.json\"\n",
    "prompt = load_json(prompt_path)\n",
    "\n",
    "for task in [\"solve\", \"unsol\"]:\n",
    "    input_file = input_path.format(task)\n",
    "    data_pool = read_jsonl(input_file)\n",
    "    # print(f\"Task: {task}, Data Size: {len(data_pool)}\")\n",
    "    new_data_list = []\n",
    "    data_pool = random.sample(data_pool, int(len(data_pool) * 0.5))\n",
    "    for data in data_pool:\n",
    "        if len(data[\"generation\"]) == 0:\n",
    "            print(f\"Data {data['id']} has no generation.\")\n",
    "            continue\n",
    "        _, judge, _ = realmath_reward_fn(data[\"generation\"][0][\"answer\"], data[\"ground_truth\"], task=task)\n",
    "        if judge == \"unknown\":\n",
    "            new_data_list.append(data)\n",
    "            continue\n",
    "    print(f\"Task: {task}, Data Size: {len(new_data_list)}\")\n",
    "    output_file = output_path.format(task)\n",
    "    write_jsonl(output_file, new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: solve, Type: success, Data Size: 5344\n",
      "Task: solve, Type: success, Data Size: 5344\n",
      "Task: solve, Type: refuse, Data Size: 481\n",
      "Task: solve, Type: refuse, Data Size: 481\n",
      "Task: unsol, Type: success, Data Size: 4191\n",
      "Task: unsol, Type: success, Data Size: 4190\n",
      "Task: unsol, Type: refuse, Data Size: 787\n",
      "Task: unsol, Type: refuse, Data Size: 787\n",
      "Student: reason, Data Size: 10802\n",
      "Task: solve, Type: success, Data Size: 5344\n",
      "Task: solve, Type: success, Data Size: 5344\n",
      "Task: solve, Type: refuse, Data Size: 481\n",
      "Task: solve, Type: refuse, Data Size: 481\n",
      "Task: unsol, Type: success, Data Size: 4191\n",
      "Task: unsol, Type: success, Data Size: 4190\n",
      "Task: unsol, Type: refuse, Data Size: 787\n",
      "Task: unsol, Type: refuse, Data Size: 787\n",
      "Student: instruct, Data Size: 10802\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/train/{}_{}.json\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/RealMath/RealMath/data/train/train_{}.json\"\n",
    "\n",
    "\n",
    "for student in [\"reason\", \"instruct\"]:\n",
    "    output_list = []\n",
    "    for task in [\"solve\", \"unsol\"]:\n",
    "        for type in [\"success\", \"refuse\"]:\n",
    "            input_file = input_path.format(task, type)\n",
    "            data_pool = read_jsonl(input_file)\n",
    "            print(f\"Task: {task}, Type: {type}, Data Size: {len(data_pool)}\")\n",
    "            new_data_list = []\n",
    "            for data in data_pool:\n",
    "                if len(data[\"generation\"]) == 0:\n",
    "                    print(f\"Data {data['id']} has no generation.\")\n",
    "                    continue\n",
    "\n",
    "                if task == \"solve\":\n",
    "                    new_data = {\n",
    "                        \"id\": data[\"id\"],\n",
    "                        \"instruction\": data[\"question\"],\n",
    "                        \"input\": prompt[\"real\"],\n",
    "                        \"output\": data[\"generation\"][0][\"answer\"] if student == \"instruct\" else data[\"generation\"][0][\"reasoning\"] + data[\"generation\"][0][\"answer\"]\n",
    "                    }\n",
    "                    new_data_list.append(new_data)\n",
    "                    continue\n",
    "                elif task == \"unsol\":\n",
    "                    if data[\"generation\"][0][\"reasoning\"] is None:\n",
    "                        continue\n",
    "                    # print(data[\"data_id\"])\n",
    "                    new_data = {\n",
    "                        \"id\": data[\"data_id\"],\n",
    "                        \"instruction\": data[\"rewritten_question\"],\n",
    "                        \"input\": prompt[\"real\"],\n",
    "                        \"output\": data[\"generation\"][0][\"answer\"] if student == \"instruct\" else data[\"generation\"][0][\"reasoning\"] + data[\"generation\"][0][\"answer\"]\n",
    "                    }\n",
    "                    new_data_list.append(new_data)\n",
    "                    continue\n",
    "            output_list.extend(new_data_list)\n",
    "            print(f\"Task: {task}, Type: {type}, Data Size: {len(new_data_list)}\")\n",
    "    output_file = output_path.format(student)\n",
    "    write_jsonl(output_file, output_list)\n",
    "    print(f\"Student: {student}, Data Size: {len(output_list)}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: solve, Data Size: 313\n",
      "Task: unsol, Data Size: 1102\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "input_path = \"../data/{}/{}.json\"\n",
    "output_path = \"../data/{}.json\"\n",
    "\n",
    "tasks = [\"solve\", \"unsol\"]\n",
    "datasets = [\"aime\", \"amc\", \"math\", \"minerva\"]\n",
    "\n",
    "for task in tasks:\n",
    "    output_file = output_path.format(task)\n",
    "    data_pool = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        input_file = input_path.format(task, dataset)\n",
    "        data_pool.extend(read_json(input_file))\n",
    "    print(f\"Task: {task}, Data Size: {len(data_pool)}\")\n",
    "    write_json(output_file, data_pool)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/ReliableMath/ReliableMath/data/{}.json\"\n",
    "output_path = \"/Users/collcertaye/WorkSpace/Research/NLP&LLM/ReliableMath/ReliableMath/data/{}.parquet\"\n",
    "tasks = [\"solve\", \"unsol\"]\n",
    "\n",
    "for task in tasks:\n",
    "    input_file = input_path.format(task)\n",
    "    data_pool = pd.read_json(input_file)\n",
    "    # print(f\"Task: {task}, Data Size: {len(data_pool)}\")\n",
    "    df = pd.DataFrame(data_pool)\n",
    "    df['ground_truth'] = df['ground_truth'].astype(str)\n",
    "    output_file = output_path.format(task)\n",
    "    df.to_parquet(output_file, engine='pyarrow', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
